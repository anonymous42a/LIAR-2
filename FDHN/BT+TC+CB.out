Fri Apr 21 00:17:15 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-PCI...  On   | 00000000:21:00.0 Off |                    0 |
| N/A   27C    P0    34W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCI...  On   | 00000000:81:00.0 Off |                    0 |
| N/A   27C    P0    36W / 250W |      0MiB / 40960MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch Version : 1.13.1
cuda
***** Best Result Updated at Epoch 1, Val Loss: 0.4069 *****
Epoch [1/10], Time: 240.92s, Train Loss: 0.4406, Train Acc: 0.2419, Train F1 Macro: 0.1976, Train F1 Micro: 0.2419, Val Loss: 0.4069, Val Acc: 0.3442, Val F1 Macro: 0.3018, Val F1 Micro: 0.3442
***** Best Result Updated at Epoch 2, Val Loss: 0.3767 *****
Epoch [2/10], Time: 236.11s, Train Loss: 0.3946, Train Acc: 0.3742, Train F1 Macro: 0.2697, Train F1 Micro: 0.3081, Val Loss: 0.3767, Val Acc: 0.4221, Val F1 Macro: 0.3710, Val F1 Micro: 0.3832
***** Best Result Updated at Epoch 3, Val Loss: 0.3596 *****
Epoch [3/10], Time: 236.43s, Train Loss: 0.3711, Train Acc: 0.4272, Train F1 Macro: 0.3232, Train F1 Micro: 0.3478, Val Loss: 0.3596, Val Acc: 0.4431, Val F1 Macro: 0.3973, Val F1 Micro: 0.4032
***** Best Result Updated at Epoch 4, Val Loss: 0.3522 *****
Epoch [4/10], Time: 236.31s, Train Loss: 0.3563, Train Acc: 0.4481, Train F1 Macro: 0.3551, Train F1 Micro: 0.3729, Val Loss: 0.3522, Val Acc: 0.4509, Val F1 Macro: 0.4109, Val F1 Micro: 0.4151
Epoch [5/10], Time: 232.05s, Train Loss: 0.3478, Train Acc: 0.4559, Train F1 Macro: 0.3762, Train F1 Micro: 0.3895, Val Loss: 0.3542, Val Acc: 0.4307, Val F1 Macro: 0.4176, Val F1 Micro: 0.4182
***** Best Result Updated at Epoch 6, Val Loss: 0.3507 *****
Epoch [6/10], Time: 236.74s, Train Loss: 0.3447, Train Acc: 0.4542, Train F1 Macro: 0.3903, Train F1 Micro: 0.4003, Val Loss: 0.3507, Val Acc: 0.4338, Val F1 Macro: 0.4214, Val F1 Micro: 0.4208
Epoch [7/10], Time: 231.69s, Train Loss: 0.3368, Train Acc: 0.4721, Train F1 Macro: 0.4029, Train F1 Micro: 0.4105, Val Loss: 0.3520, Val Acc: 0.4361, Val F1 Macro: 0.4240, Val F1 Micro: 0.4230
Epoch [8/10], Time: 232.12s, Train Loss: 0.3315, Train Acc: 0.4899, Train F1 Macro: 0.4147, Train F1 Micro: 0.4204, Val Loss: 0.3578, Val Acc: 0.4167, Val F1 Macro: 0.4235, Val F1 Micro: 0.4222
Epoch [9/10], Time: 232.07s, Train Loss: 0.3254, Train Acc: 0.4985, Train F1 Macro: 0.4248, Train F1 Micro: 0.4291, Val Loss: 0.3569, Val Acc: 0.4338, Val F1 Macro: 0.4246, Val F1 Micro: 0.4235
Epoch [10/10], Time: 231.95s, Train Loss: 0.3191, Train Acc: 0.5125, Train F1 Macro: 0.4344, Train F1 Micro: 0.4375, Val Loss: 0.3563, Val Acc: 0.4245, Val F1 Macro: 0.4245, Val F1 Micro: 0.4236
Total Training Time: 2346.39s
Test Loss: 0.3624, Test Acc: 0.4081, Test F1 Macro: 0.4152, Test F1 Micro: 0.4081
